{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147c78a3",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ee1d6",
   "metadata": {},
   "source": [
    "### Trim video based on timestamp CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2264ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def time_to_seconds(t):\n",
    "    parts = t.split(':')\n",
    "    parts = [float(p) for p in parts]\n",
    "    if len(parts) == 3:  # HH:MM:SS\n",
    "        return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
    "    elif len(parts) == 2:  # MM:SS\n",
    "        return parts[0] * 60 + parts[1]\n",
    "    else:\n",
    "        return parts[0]\n",
    "\n",
    "def generate_ffmpeg_commands(df, label):\n",
    "    commands = []\n",
    "    for i, row in df.iterrows():\n",
    "        start_sec = time_to_seconds(row[\"start_time\"])\n",
    "        end_sec = time_to_seconds(row[\"end_time\"])\n",
    "        duration = end_sec - start_sec\n",
    "        outname = f\"{label}_clip_{i:03d}.mp4\"\n",
    "        cmd = f'ffmpeg -i cycling1080p.mov -ss {start_sec:.2f} -t {duration:.2f} -c:v libx264 -an clips/{outname}'\n",
    "        commands.append(cmd)\n",
    "    return commands\n",
    "\n",
    "# Load CSVs\n",
    "signal_df = pd.read_csv(\"./signal.csv\")\n",
    "nosignal_df = pd.read_csv(\"./nosignal.csv\")\n",
    "\n",
    "# Generate FFmpeg commands\n",
    "signal_cmds = generate_ffmpeg_commands(signal_df, \"signal\")\n",
    "nosignal_cmds = generate_ffmpeg_commands(nosignal_df, \"nosignal\")\n",
    "\n",
    "# Save all commands to a bash script\n",
    "with open(\"extract_clips.sh\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(signal_cmds + nosignal_cmds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d555d8",
   "metadata": {},
   "source": [
    "### Determine cropping coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ace166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n",
      "Selected crop: x=856, y=226, width=1052, height=848\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the first video from ./clips (just pick one sample clip)\n",
    "clip_path = \"./clips\"\n",
    "first_clip = sorted(os.listdir(clip_path))[0]\n",
    "video_path = os.path.join(clip_path, first_clip)\n",
    "\n",
    "# Read the first frame\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "# Select ROI interactively\n",
    "roi = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "x, y, w, h = roi\n",
    "print(f\"Selected crop: x={x}, y={y}, width={w}, height={h}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6e692",
   "metadata": {},
   "source": [
    "### Run bash script to crop video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crop region\n",
    "x, y, w, h = 856, 226, 1052, 848\n",
    "\n",
    "# Directories\n",
    "input_dir = \"./clips\"\n",
    "output_dir = \"./clips_cropped\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate crop commands\n",
    "for fname in sorted(os.listdir(input_dir)):\n",
    "    if fname.endswith(\".mp4\"):\n",
    "        in_path = os.path.join(input_dir, fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        cmd = f'ffmpeg -i \"{in_path}\" -filter:v \"crop={w}:{h}:{x}:{y}\" -c:a copy \"{out_path}\"'\n",
    "        os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5766e",
   "metadata": {},
   "source": [
    "### ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffedd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "num = \"100\"\n",
    "video_path = \"./clips_cropped/signal_clip_\" + num + \".mp4\"\n",
    "tracks_path = \"./pose_keypoints/signal_clip_\" + num + \"_tracks.json\"\n",
    "output_path = \"./clips_cropped/signal_clip_\" + num + \"_tracks_overlay.mp4\"\n",
    "\n",
    "# Load YOLOv8 with tracking (must be detection model, not pose)\n",
    "model = YOLO(\"yolov8s.pt\")  # NOT yolov8n-pose.pt\n",
    "\n",
    "video_path = \"./clips_cropped/signal_clip_\" + num + \".mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_id = 0\n",
    "detections = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Track=True enables ByteTrack\n",
    "    results = model.track(frame, persist=True, classes=[0], tracker=\"bytetrack.yaml\")\n",
    "\n",
    "    # Only keep 'person' detections (class 0)\n",
    "    for box, track_id in zip(results[0].boxes.xyxy, results[0].boxes.id):\n",
    "        if track_id is None:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        detections.append({\n",
    "            \"frame\": frame_id,\n",
    "            \"id\": int(track_id),\n",
    "            \"bbox\": [x1, y1, x2, y2]\n",
    "        })\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save detections for pose tracking\n",
    "with open(tracks_path, \"w\") as f:\n",
    "    json.dump(detections, f)\n",
    "\n",
    "# --- --- --- --- --- --- --- --- ---\n",
    "# --- --- --- --- --- --- --- --- ---\n",
    "# --- --- --- --- --- --- --- --- ---\n",
    "\n",
    "# Load tracking data\n",
    "with open(tracks_path, \"r\") as f:\n",
    "    all_detections = json.load(f)\n",
    "\n",
    "# Index by frame\n",
    "track_by_frame = {}\n",
    "for det in all_detections:\n",
    "    frame = det[\"frame\"]\n",
    "    if frame not in track_by_frame:\n",
    "        track_by_frame[frame] = []\n",
    "    track_by_frame[frame].append(det)\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n",
    "\n",
    "frame_id = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    detections = track_by_frame.get(frame_id, [])\n",
    "\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = map(int, det[\"bbox\"])\n",
    "        track_id = det[\"id\"]\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a764dc9",
   "metadata": {},
   "source": [
    "### Tracking-guided pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b129fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Paths\n",
    "video_path = \"./clips_cropped/signal_clip_\" + num + \".mp4\"\n",
    "tracks_path = \"./pose_keypoints/signal_clip_\" + num + \"_tracks.json\"\n",
    "output_path = \"./pose_keypoints/signal_clip_\" + num + \"_pose_id1.json\"\n",
    "\n",
    "# Constants\n",
    "target_id = 1\n",
    "pose_model = YOLO(\"yolov8s-pose.pt\")\n",
    "\n",
    "# Load tracking data\n",
    "with open(tracks_path, \"r\") as f:\n",
    "    all_detections = json.load(f)\n",
    "\n",
    "# Index by frame\n",
    "track_by_frame = {}\n",
    "for det in all_detections:\n",
    "    if det[\"id\"] == target_id:\n",
    "        frame = det[\"frame\"]\n",
    "        track_by_frame[frame] = det[\"bbox\"]  # only keep 1 box per frame for this ID\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_id = 0\n",
    "pose_results = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_id in track_by_frame:\n",
    "        H, W = frame.shape[:2]\n",
    "        x1, y1, x2, y2 = track_by_frame[frame_id]\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        # Expand amount in each direction\n",
    "        threshold = 0.35 # percent expansion of box\n",
    "        margin_x = threshold * w\n",
    "        margin_y = threshold * h\n",
    "\n",
    "        x1_exp = max(0, int(x1 - margin_x))\n",
    "        y1_exp = max(0, int(y1 - margin_y))\n",
    "        x2_exp = min(W, int(x2 + margin_x))\n",
    "        y2_exp = min(H, int(y2 + margin_y))\n",
    "\n",
    "        crop = frame[y1_exp:y2_exp, x1_exp:x2_exp]\n",
    "\n",
    "        # Pose estimation on cropped person\n",
    "        results = pose_model(crop)\n",
    "        if results and results[0].keypoints.xy is not None:\n",
    "            keypoints = results[0].keypoints.xy[0].cpu().tolist()\n",
    "            # Convert to full-frame coords\n",
    "            keypoints_full = [[x + x1_exp, y + y1_exp] for x, y in keypoints]\n",
    "        else:\n",
    "            keypoints_full = []\n",
    "    else:\n",
    "        keypoints_full = []\n",
    "\n",
    "    pose_results.append({\n",
    "        \"frame\": frame_id,\n",
    "        \"keypoints\": keypoints_full\n",
    "    })\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(pose_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5128c3",
   "metadata": {},
   "source": [
    "### Overlay the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "video_path = \"./clips_cropped/signal_clip_\" + num + \".mp4\"\n",
    "keypoint_path = \"./pose_keypoints/signal_clip_\" + num + \"_pose_id1.json\"\n",
    "output_path = \"./clips_cropped/signal_clip_\" + num + \"_pose_id1_overlay.mp4\"\n",
    "\n",
    "# Load pose data\n",
    "with open(keypoint_path, \"r\") as f:\n",
    "    pose_data = json.load(f)\n",
    "\n",
    "# COCO skeleton edges\n",
    "skeleton = [\n",
    "    (5, 7), (7, 9),  # Left arm\n",
    "    (6, 8), (8, 10), # Right arm\n",
    "    (5, 6), (5, 11), (6, 12),\n",
    "    (11, 13), (13, 15),\n",
    "    (12, 14), (14, 16),\n",
    "    (11, 12)\n",
    "]\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n",
    "\n",
    "frame_id = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame_id >= len(pose_data):\n",
    "        break\n",
    "\n",
    "    keypoints = pose_data[frame_id][\"keypoints\"]\n",
    "\n",
    "    if keypoints and len(keypoints) == 17:\n",
    "        # Draw joints\n",
    "        for x, y in keypoints:\n",
    "            cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw skeleton\n",
    "        for i, j in skeleton:\n",
    "            x1, y1 = keypoints[i]\n",
    "            x2, y2 = keypoints[j]\n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13ba62",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the pose data\n",
    "with open(\"./pose_keypoints/signal_clip_\" + num + \"_pose_id1.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# COCO indices\n",
    "right_wrist_idx = 4\n",
    "right_hip_idx = 12\n",
    "left_hip_idx = 11\n",
    "\n",
    "# Compute lateral (orthogonal to sagittal plane) distance\n",
    "lateral_distances = []\n",
    "\n",
    "for frame_data in data:\n",
    "    keypoints = frame_data[\"keypoints\"]\n",
    "    frame_number = frame_data[\"frame\"]\n",
    "\n",
    "    # if len(keypoints) > max(right_wrist_idx, right_hip_idx):\n",
    "    #     lateral_distance = np.linalg.norm(keypoints[right_wrist_idx][0])\n",
    "    if len(keypoints) > max(right_wrist_idx, right_hip_idx, left_hip_idx):\n",
    "        rw = np.array(keypoints[right_wrist_idx])\n",
    "        rh = np.array(keypoints[right_hip_idx])\n",
    "        lh = np.array(keypoints[left_hip_idx])\n",
    "\n",
    "        # Hip-to-hip vector defines the lateral (left-right) axis\n",
    "        hip_vec = rh - lh\n",
    "        hip_vec_unit = hip_vec / (np.linalg.norm(hip_vec) + 1e-6)\n",
    "\n",
    "        # Right wrist vector relative to right hip\n",
    "        rw_vec = rw - rh\n",
    "\n",
    "        # Orthogonal component (how far out laterally is the wrist from hip-hip line)\n",
    "        projection = np.dot(rw_vec, hip_vec_unit) * hip_vec_unit\n",
    "        orthogonal = rw_vec - projection\n",
    "        lateral_distance = np.linalg.norm(orthogonal)\n",
    "    else:\n",
    "        lateral_distance = None\n",
    "\n",
    "    lateral_distances.append({\n",
    "        \"frame\": frame_number,\n",
    "        \"lateral_distance_rw_from_hipline\": lateral_distance\n",
    "    })\n",
    "\n",
    "# Prepare for plotting\n",
    "frames = [d[\"frame\"] for d in lateral_distances if d[\"lateral_distance_rw_from_hipline\"] is not None]\n",
    "lateral_vals = [d[\"lateral_distance_rw_from_hipline\"] for d in lateral_distances if d[\"lateral_distance_rw_from_hipline\"] is not None]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(frames, lateral_vals)\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Distance (pixels)\")\n",
    "plt.title(\"Right Wrist Lateral Distance from Hip\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1346d",
   "metadata": {},
   "source": [
    "Savitzky-Golay Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffe2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from lateral distances\n",
    "df_lat = pd.DataFrame(lateral_distances)\n",
    "df_lat = df_lat.dropna()\n",
    "\n",
    "# Apply Savitzky-Golay filter\n",
    "df_lat[\"smoothed\"] = savgol_filter(df_lat[\"lateral_distance_rw_from_hipline\"], window_length=30, polyorder=2, mode='interp')\n",
    "\n",
    "# Plot original vs smoothed\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(df_lat[\"frame\"], df_lat[\"smoothed\"], linewidth=2, color=\"orange\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Lateral Distance (pixels)\")\n",
    "plt.title(\"Right Wrist Lateral Distance from Hip Line (Smoothed)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2afba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Load video\n",
    "video_path = \"clips_cropped/signal_clip_\"+num+\"_pose_id1_overlay.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "W, H = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Setup matplotlib plot\n",
    "smoothed = df_lat[\"smoothed\"].values\n",
    "fig, ax = plt.subplots(figsize=(8, 3.18))\n",
    "ax.plot(frames, smoothed, color='orange')\n",
    "line, = ax.plot([0, 0], [min(smoothed), max(smoothed)], 'r-', lw=2)\n",
    "ax.set_title(\"Right Wrist Lateral Distance\")\n",
    "ax.set_xlabel(\"Frame\")\n",
    "ax.set_ylabel(\"Distance\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.draw()\n",
    "\n",
    "# Writer\n",
    "out_path = \"clips_cropped/signal_clip_\"+num+\"_combined_overlay.mp4\"\n",
    "plot_height = 400\n",
    "writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H + plot_height))\n",
    "\n",
    "frame_id = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame_id >= len(smoothed):\n",
    "        break\n",
    "\n",
    "    # Update line\n",
    "    line.set_xdata([frame_id, frame_id])\n",
    "    fig.canvas.draw()\n",
    "    plot_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    plot_img = plot_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plot_img = cv2.resize(plot_img, (W, plot_height))\n",
    "\n",
    "    # Combine\n",
    "    combined = np.vstack([frame, plot_img])\n",
    "    writer.write(combined)\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE460",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
